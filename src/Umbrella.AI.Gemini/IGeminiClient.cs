using System;
using System.Collections.Generic;
using System.Linq;
using System.Threading.Tasks;
using Umbrella.AI.Gemini.Models;

namespace Umbrella.AI.Gemini
{
    /// <summary>
    /// Abstraction of a client of REST api of Gemini AI
    /// </summary>A
    public interface IGeminiClient
    {
        /// <summary>
        /// Generates a Text Response using a specific Gemini Model.
        /// </summary>
        /// <param name="modelName">The name of the Gemini model to use (e.g., "gemini-pro").</param>
        /// <param name="prompt">The input text for the model.</param>
        /// <param name="temperature">Controls the randomness of the generated response (between 0.0 and 1.0).</param>
        /// <param name="maxTokens">The maximum number of tokens to generate in the response.</param>
        /// <param name="topP">Controls the top-P sampling probability. A higher value means more randomness, while a lower value means more deterministic output. The default value is 0.95.</param>
        /// <param name="topK">Controls the top-K sampling probability. A higher value means more randomness, while a lower value means more deterministic output. The default value is 40.</param  
        /// <param name="cancellationToken">Token to cancel the request.</param>
        /// <returns>Response generated by the model.</returns>
        Task<GeminiResponse> GenerateContentAsync(
            string modelName,
            string prompt,
            float temperature = 0.7f,
            float topP = 0.95f,
            int topK = 40,
            int? maxTokens = null,
            CancellationToken cancellationToken = default);
        /// <summary>
        /// Generates a Text Response using a specific Gemini Model.
        /// </summary>
        /// <param name="modelName">The name of the Gemini model to use (e.g., "gemini-pro").</param>
        /// <param name="prompt">The input text for the model.</param>
        /// <param name="systemInstruction">A system instruction to guide the model's behavior. This instruction should be a JSON object.</param>
        /// <param name="temperature">Controls the randomness of the generated response (between 0.0 and 1.0).</param>
        /// <param name="maxTokens">The maximum number of tokens to generate in the response.</param>
        /// <param name="topP">Controls the top-P sampling probability. A higher value means more randomness, while a lower value means more deterministic output. The default value is 0.95.</param>
        /// <param name="topK">Controls the top-K sampling probability. A higher value means more randomness, while a lower value means more deterministic output. The default value is 40.</param  
        /// <param name="cancellationToken">Token to cancel the request.</param>
        /// <returns>Response generated by the model.</returns>
        Task<GeminiResponse> GenerateContentWithContextAsync(
            string modelName,
            string prompt,
            string systemInstruction,
            float temperature = 0.7f,
            float topP = 0.95f,
            int topK = 40,
            int? maxTokens = null,
            CancellationToken cancellationToken = default);
        /// <summary>
        /// Lists available Gemini models.
        /// </summary>
        /// <param name="cancellationToken">Token to cancel the request.</param>
        /// <returns>List of available models.</returns>
        /// <returns>Response generated by the model.</returns>
        Task<ModelsResponse> ListModelsAsync(CancellationToken cancellationToken = default);
        /// <summary>
        /// Gets information about a specific model.
        /// </summary>
        /// <param name="modelName">The name of the model (e.g., "gemini-pro").</param>
        /// <param name="cancellationToken">Token to cancel the request.</param>
        /// <returns>Information about the model.</returns>
        Task<ModelInfo> GetModelInfoAsync(string modelName, CancellationToken cancellationToken = default);
        /// <summary>
        /// Counts tokens in a text.
        /// </summary>
        /// <param name="modelName">The model name.</param>
        /// <param name="text">The text to count tokens from.</param>
        /// <param name="cancellationToken">Cancellation token for the request.</param>
        /// <returns>Token count.</returns>
        Task<CountTokensResponse> CountTokensAsync(
            string modelName,
            string text,
            CancellationToken cancellationToken = default);
    }
}