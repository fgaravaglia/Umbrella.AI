using System.Net.Http.Headers;
using System.Text;
using System.Text.Json;
using Microsoft.Extensions.Logging;
using Umbrella.AI.Gemini.Models;

namespace Umbrella.AI.Gemini
{
    /// <summary>
    /// Client per le API di Google Gemini che implementa IDisposable.
    /// </summary>
    public class GeminiClient : IGeminiClient, IDisposable
    {
        #region Fields
        private readonly HttpClient _httpClient;
        private readonly string _apiKey;
        private bool _disposed;
        private readonly ILogger _Logger;
        #endregion

        /// <summary>
        /// URL base per le API di Gemini.
        /// </summary>
        private const string BaseUrlV1Beta = "https://generativelanguage.googleapis.com/v1beta/";

        /// <summary>
        /// Instances new  <see cref="GeminiClient"/>.
        /// </summary>
        /// <param name="logger">THe logger</param>
        /// <param name="apiKey">Api Key for Google Gemini.</param>
        /// <param name="httpClient">Optional. pre-existing HttpClient instance; if null, it is instanciated from scratch</param>
        public GeminiClient(ILogger logger, string apiKey, HttpClient? httpClient = null)
        {
            this._Logger = logger ?? throw new ArgumentNullException(nameof(logger));
            if (string.IsNullOrWhiteSpace(apiKey))
                throw new ArgumentException("La chiave API non può essere vuota o null.", nameof(apiKey));

            _apiKey = apiKey;
            _httpClient = httpClient ?? new HttpClient();
            _httpClient.BaseAddress = new Uri(BaseUrlV1Beta);
        }

        /// <summary>
        /// Generates a Text Response using a specific Gemini Model.
        /// </summary>
        /// <param name="modelName">The name of the Gemini model to use (e.g., "gemini-pro").</param>
        /// <param name="prompt">The input text for the model.</param>
        /// <param name="temperature">Controls the randomness of the generated response (between 0.0 and 1.0).</param>
        /// <param name="maxTokens">The maximum number of tokens to generate in the response.</param>
        /// <param name="cancellationToken">Token to cancel the request.</param>
        /// <returns>Response generated by the model.</returns>
        public async Task<GeminiResponse> GenerateContentAsync(
            string modelName,
            string prompt,
            float temperature = 0.7f,
            int? maxTokens = null,
            CancellationToken cancellationToken = default)
        {
            ThrowIfDisposed();
            //https://ai.google.dev/gemini-api/docs/text-generation?hl=it&lang=rest#system-instructions
            var requestData = new
            {
                contents = new[]
                {
                    new
                    {
                        parts = new[]
                        {
                            new
                            {
                                text = prompt
                            }
                        }
                    }
                },
                generationConfig = new
                {
                    // controlla la casualità dell'output. 
                    // Utilizza valori più elevati per risposte più creative e valori più bassi per risposte più deterministiche. 
                    // I valori possono essere compresi tra [0,0, 2,0].
                    temperature,
                    maxOutputTokens = maxTokens, // imposta il numero massimo di token da includere in un candidato.
                    //cambia il modo in cui il modello seleziona i token per l'output. 
                    //I token vengono selezionati dal più probabile al meno probabile finché la somma delle loro probabilità non corrisponde 
                    // al valore topP. Il valore predefinito di topP è 0,95.
                    topP = 0.95,
                    topK = 40 // cambia il modo in cui il modello seleziona i token per l'output. 
                }
            };

            var requestUrl = $"models/{modelName}:generateContent?key={_apiKey}";
            var response = await SendPostRequestAsync<GeminiResponse>(requestUrl, requestData, cancellationToken).ConfigureAwait(false);

            return response;
        }

        /// <summary>
        /// Generates a Text Response using a specific Gemini Model.
        /// </summary>
        /// <param name="modelName">The name of the Gemini model to use (e.g., "gemini-pro").</param>
        /// <param name="prompt">The input text for the model.</param>
        /// <param name="temperature">Controls the randomness of the generated response (between 0.0 and 1.0).</param>
        /// <param name="maxTokens">The maximum number of tokens to generate in the response.</param>
        /// <param name="topP">Controls the top-P sampling probability. A higher value means more randomness, while a lower value means more deterministic output. The default value is 0.95.</param>
        /// <param name="topK">Controls the top-K sampling probability. A higher value means more randomness, while a lower value means more deterministic output. The default value is 40.</param  
        /// <param name="cancellationToken">Token to cancel the request.</param>
        /// <returns>Response generated by the model.</returns>
        public async Task<GeminiResponse> GenerateContentAsync(
            string modelName,
            string prompt,
            float temperature = 0.7f,
            float topP = 0.95f,
            int topK = 40,
            int? maxTokens = null,
            CancellationToken cancellationToken = default)
        {
            ThrowIfDisposed();

            ArgumentNullException.ThrowIfNullOrEmpty(modelName);
            ArgumentNullException.ThrowIfNullOrEmpty(prompt);
            if (temperature < 0 || temperature > 1)
                throw new ArgumentOutOfRangeException(nameof(temperature), "Temperature must be between 0 and 1");
            if (topP < 0 || topP > 1)
                throw new ArgumentOutOfRangeException(nameof(topP), "TopP must be between 0 and 1");
            //https://ai.google.dev/gemini-api/docs/text-generation?hl=it&lang=rest#system-instructions
            var requestData = new
            {
                contents = new[]
                {
                    new
                    {
                        parts = new[]
                        {
                            new
                            {
                                text = prompt
                            }
                        }
                    }
                },
                generationConfig = new
                {
                    // controlla la casualità dell'output. 
                    // Utilizza valori più elevati per risposte più creative e valori più bassi per risposte più deterministiche. 
                    // I valori possono essere compresi tra [0,0, 2,0].
                    temperature,
                    maxOutputTokens = maxTokens, // imposta il numero massimo di token da includere in un candidato.
                    //cambia il modo in cui il modello seleziona i token per l'output. 
                    //I token vengono selezionati dal più probabile al meno probabile finché la somma delle loro probabilità non corrisponde 
                    // al valore topP. Il valore predefinito di topP è 0,95.
                    topP = topP,
                    topK = topK // cambia il modo in cui il modello seleziona i token per l'output. 
                }
            };

            var requestUrl = $"models/{modelName}:generateContent?key={_apiKey}";
            var response = await SendPostRequestAsync<GeminiResponse>(requestUrl, requestData, cancellationToken).ConfigureAwait(false);

            return response;
        }
        /// <summary>
        /// Generates a Text Response using a specific Gemini Model.
        /// </summary>
        /// <param name="modelName">The name of the Gemini model to use (e.g., "gemini-pro").</param>
        /// <param name="prompt">The input text for the model.</param>
        /// <param name="systemInstruction">A system instruction to guide the model's behavior. This instruction should be a JSON object.</param>
        /// <param name="temperature">Controls the randomness of the generated response (between 0.0 and 1.0).</param>
        /// <param name="maxTokens">The maximum number of tokens to generate in the response.</param>
        /// <param name="topP">Controls the top-P sampling probability. A higher value means more randomness, while a lower value means more deterministic output. The default value is 0.95.</param>
        /// <param name="topK">Controls the top-K sampling probability. A higher value means more randomness, while a lower value means more deterministic output. The default value is 40.</param  
        /// <param name="cancellationToken">Token to cancel the request.</param>
        /// <returns>Response generated by the model.</returns>
        public async Task<GeminiResponse> GenerateContentWithContextAsync(
            string modelName,
            string prompt,
            string systemInstruction,
            float temperature = 0.7f,
            float topP = 0.95f,
            int topK = 40,
            int? maxTokens = null,
            CancellationToken cancellationToken = default)
        {
            ThrowIfDisposed();
            ArgumentNullException.ThrowIfNullOrEmpty(modelName);
            ArgumentNullException.ThrowIfNullOrEmpty(prompt);
            ArgumentNullException.ThrowIfNullOrEmpty(systemInstruction);
            if (temperature < 0 || temperature > 1)
                throw new ArgumentOutOfRangeException(nameof(temperature), "Temperature must be between 0 and 1");
            if (topP < 0 || topP > 1)
                throw new ArgumentOutOfRangeException(nameof(topP), "TopP must be between 0 and 1");

            //https://ai.google.dev/gemini-api/docs/text-generation?hl=it&lang=rest#system-instructions
            var requestData = new
            {
                system_instruction = new
                {
                    parts = new[]
                        {
                            new { text = prompt }
                        }
                },
                contents = new[]{
                    new
                    {
                        parts = new[]
                        {
                            new { text = prompt }
                        }
                    }
                },
                generationConfig = new
                {
                    // controlla la casualità dell'output. 
                    // Utilizza valori più elevati per risposte più creative e valori più bassi per risposte più deterministiche. 
                    // I valori possono essere compresi tra [0,0, 2,0].
                    temperature,
                    maxOutputTokens = maxTokens, // imposta il numero massimo di token da includere in un candidato.
                    //cambia il modo in cui il modello seleziona i token per l'output. 
                    //I token vengono selezionati dal più probabile al meno probabile finché la somma delle loro probabilità non corrisponde 
                    // al valore topP. Il valore predefinito di topP è 0,95.
                    topP = topP,
                    topK = topK // cambia il modo in cui il modello seleziona i token per l'output. 
                }
            };

            var requestUrl = $"models/{modelName}:generateContent?key={_apiKey}";
            var response = await SendPostRequestAsync<GeminiResponse>(requestUrl, requestData, cancellationToken).ConfigureAwait(false);

            return response;
        }

        /// <summary>
        /// Lists available Gemini models.
        /// </summary>
        /// <param name="cancellationToken">Token to cancel the request.</param>
        /// <returns>List of available models.</returns>
        /// <returns>Response generated by the model.</returns>
        public async Task<ModelsResponse> ListModelsAsync(CancellationToken cancellationToken = default)
        {
            ThrowIfDisposed();

            var requestUrl = $"models?key={_apiKey}";
            var response = await this.SendGetRequestAsync<ModelsResponse>(requestUrl, cancellationToken).ConfigureAwait(false);
            return response;
        }

        /// <summary>
        /// Gets information about a specific model.
        /// </summary>
        /// <param name="modelName">The name of the model (e.g., "gemini-pro").</param>
        /// <param name="cancellationToken">Token to cancel the request.</param>
        /// <returns>Information about the model.</returns>
        public async Task<ModelInfo> GetModelInfoAsync(string modelName, CancellationToken cancellationToken = default)
        {
            ThrowIfDisposed();

            var requestUrl = $"models/{modelName}?key={_apiKey}";
            var response = await SendGetRequestAsync<ModelInfo>(requestUrl, cancellationToken).ConfigureAwait(false);
            return response;
        }

        /// <summary>
        /// Counts tokens in a text.
        /// </summary>
        /// <param name="modelName">The model name.</param>
        /// <param name="text">The text to count tokens from.</param>
        /// <param name="cancellationToken">Cancellation token for the request.</param>
        /// <returns>Token count.</returns>
        public async Task<CountTokensResponse> CountTokensAsync(
            string modelName,
            string text,
            CancellationToken cancellationToken = default)
        {
            ThrowIfDisposed();


            var requestData = new
            {
                contents = new[]
                {
                    new
                    {
                        parts = new[]
                        {
                            new
                            {
                                text
                            }
                        }
                    }
                }
            };

            var requestUrl = $"models/{modelName}:countTokens?key={_apiKey}";
            var response = await SendPostRequestAsync<CountTokensResponse>(requestUrl, requestData, cancellationToken).ConfigureAwait(false);

            return response;
        }

        #region Private methods
        /// <summary>
        /// Generic method to consume GET endpoint
        /// </summary>
        /// <typeparam name="T"></typeparam>
        /// <param name="relativeUrl"></param>
        /// <param name="cancellationToken"></param>
        /// <returns></returns>
        private async Task<T> SendGetRequestAsync<T>(string relativeUrl, CancellationToken cancellationToken)
        {
            var request = new HttpRequestMessage(HttpMethod.Get, relativeUrl);
            request.Headers.Accept.Add(new MediaTypeWithQualityHeaderValue("application/json"));
            var response = await _httpClient.SendAsync(request, cancellationToken).ConfigureAwait(false);
            response.EnsureSuccessStatusCode();
            var responseContent = await response.Content.ReadAsStringAsync().ConfigureAwait(false);
            this._Logger.LogInformation("Url: GET {RelativeUrl}", relativeUrl);
            this._Logger.LogInformation("REsponse: {Response}", responseContent);
            var result = JsonSerializer.Deserialize<T>(responseContent, new JsonSerializerOptions
            {
                PropertyNameCaseInsensitive = true
            });
            return result ?? default(T);
        }
        /// <summary>
        /// GEenric method to consume POST endpoint
        /// </summary>
        private async Task<T> SendPostRequestAsync<T>(string relativeUrl, object requestData, CancellationToken cancellationToken)
        {
            ArgumentNullException.ThrowIfNull(relativeUrl);
            ArgumentNullException.ThrowIfNull(requestData);

            var request = new HttpRequestMessage(HttpMethod.Post, relativeUrl);
            request.Headers.Accept.Add(new MediaTypeWithQualityHeaderValue("application/json"));

            // Setting the Body of request
            this._Logger.LogInformation("Url: POST {RelativeUrl}", relativeUrl);
            if (requestData != null)
            {
                var jsonContent = JsonSerializer.Serialize(requestData, new JsonSerializerOptions
                {
                    PropertyNamingPolicy = JsonNamingPolicy.CamelCase
                });
                this._Logger.LogInformation("REquest: {Request}", jsonContent);
                request.Content = new StringContent(jsonContent, Encoding.UTF8, "application/json");
            }

            // Sendind the request to endpoint
            var response = await _httpClient.SendAsync(request, cancellationToken).ConfigureAwait(false);
            response.EnsureSuccessStatusCode();

            var responseContent = await response.Content.ReadAsStringAsync().ConfigureAwait(false);
            this._Logger.LogInformation("Response: {Response}", responseContent);
            var result = JsonSerializer.Deserialize<T>(responseContent, new JsonSerializerOptions
            {
                PropertyNameCaseInsensitive = true,
            });
            return result ?? default(T);

        }
        /// <summary>
        /// CHeck if object has been dismissed
        /// </summary>
        private void ThrowIfDisposed()
        {
            if (_disposed)
                throw new ObjectDisposedException(nameof(GeminiClient));
        }
        #endregion

        #region IDispose
        /// <summary>
        /// Free resources from thread
        /// </summary>
        public void Dispose()
        {
            Dispose(true);
            GC.SuppressFinalize(this);
        }

        /// <summary>
        /// Releases the resources used by the client.
        /// </summary>
        /// <param name="disposing">Indicates whether the method was called by Dispose().</param>
        protected virtual void Dispose(bool disposing)
        {
            if (_disposed)
                return;

            if (disposing)
            {
                _httpClient?.Dispose();
            }

            _disposed = true;
        }

        /// <summary>
        /// 
        /// </summary>
        ~GeminiClient()
        {
            Dispose(false);
        }
        #endregion
    }
}